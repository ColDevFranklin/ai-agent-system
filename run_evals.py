from evals.workflow_evaluator import WorkflowEvaluator
from workflows.multi_agent_orchestrator import MultiAgentOrchestrator
import json

def print_section(title):
    """Helper para imprimir secciones visuales"""
    print("\n" + "=" * 60)
    print(title)
    print("=" * 60)

def main():
    # Crear evaluador y orquestador
    print("üîß Inicializando sistema de evaluaci√≥n...")
    evaluator = WorkflowEvaluator()
    orchestrator = MultiAgentOrchestrator()

    # Ejecutar tests
    results = evaluator.run_all_tests(orchestrator)

    # Generar reporte
    report = evaluator.generate_report(results)

    # SECCI√ìN 1: Resumen ejecutivo
    print_section("üìà RESUMEN EJECUTIVO")
    print(f"\nTests totales:    {report['total_tests']}")
    print(f"Tests exitosos:   {report['passed']} ‚úÖ")
    print(f"Tests fallidos:   {report['failed']} ‚ùå")
    print(f"Tasa de √©xito:    {report['pass_rate']:.1f}%")

    # SECCI√ìN 2: Checks problem√°ticos
    if report['failed_checks']:
        print_section("‚ö†Ô∏è COMPONENTES CON PROBLEMAS")
        print("\nVerificaciones que fallaron m√°s frecuentemente:\n")
        for check_name, count in sorted(report['failed_checks'].items(),
                                       key=lambda x: x[1],
                                       reverse=True):
            print(f"  ‚Ä¢ {check_name}: {count} fallas")

    # SECCI√ìN 3: Detalle por test
    print_section("üîç DETALLE POR TEST")

    for r in report['results']:
        status_icon = "‚úÖ" if r['passed'] else "‚ùå"
        print(f"\n{status_icon} {r['test_id']}: {r['nombre']}")
        print(f"   Descripci√≥n: {r['descripcion']}")
        print(f"   Score: {r['score']} ({r['percentage']:.0f}%)")

        if not r['passed']:
            print("   Verificaciones:")
            for check_name, check_data in r['checks'].items():
                check_icon = "‚úÖ" if check_data['pass'] else "‚ùå"
                print(f"     {check_icon} {check_data['description']}")
                if not check_data['pass']:
                    print(f"        ‚Üí Esperado: {check_data['expected']}")
                    print(f"        ‚Üí Actual: {check_data['actual']}")

    # SECCI√ìN 4: Recomendaciones
    print_section("üí° RECOMENDACIONES")

    if report['pass_rate'] == 100:
        print("\nüéâ ¬°Excelente! Todos los tests pasaron.")
        print("Considera agregar m√°s casos edge:")
        print("  ‚Ä¢ M√∫ltiples √≥rdenes en un mensaje")
        print("  ‚Ä¢ Direcciones internacionales")
        print("  ‚Ä¢ Caracteres especiales en nombres")

    elif report['pass_rate'] >= 66:
        print("\nüëç Sistema funcional con √°reas de mejora.")
        print("Prioriza arreglar:")
        if 'error_handled' in report['failed_checks']:
            print("  ‚Ä¢ Manejo de errores m√°s robusto")
        if 'graceful_failure' in report['failed_checks']:
            print("  ‚Ä¢ Mensajes de error m√°s claros al usuario")

    else:
        print("\n‚ö†Ô∏è Sistema requiere atenci√≥n urgente.")
        print("Problemas cr√≠ticos detectados:")
        for check_name in report['failed_checks']:
            print(f"  ‚Ä¢ {check_name}")
        print("\nRevisa logs de ejecuci√≥n en detalle.")

    # SECCI√ìN 5: Exportar resultados
    print_section("üíæ EXPORTANDO RESULTADOS")

    output_file = "eval_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"\n‚úÖ Resultados guardados en: {output_file}")
    print("   √ösalo para trackear mejoras a lo largo del tiempo")

if __name__ == "__main__":
    main()
